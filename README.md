# ğŸ¤– Ollama UI

A modern, feature-rich user interface for Ollama, providing a seamless experience for interacting with local language models. Built with Next.js 15, TypeScript, and Tailwind CSS.

[![Next.js](https://img.shields.io/badge/next.js-15.1.7-black.svg)](https://nextjs.org)
[![TypeScript](https://img.shields.io/badge/typescript-5.0.0-blue.svg)](https://www.typescriptlang.org)
[![Tailwind CSS](https://img.shields.io/badge/tailwindcss-3.4.1-38bdf8.svg)](https://tailwindcss.com)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

## ğŸ¯ Why Ollama UI?

AI is rapidly evolving with powerful language models like Llama, DeepSeek, Mistral, and Phi now capable of running on consumer-grade GPUs. Tools like Ollama make this possible, but there's still a significant barrier to entry - command line interfaces, AI terminology, and complex model parameters can be intimidating for newcomers.

Ollama UI bridges this gap by providing an intuitive, user-friendly interface that serves as a playground for both beginners and experts. Whether you're running models locally on your machine or hosting in the cloud to share with others, our goal is to make AI exploration accessible to everyone.

## ğŸŒŸ Features

### ğŸ” Authentication
- Secure login system
- Protected routes and API endpoints
- JWT-based authentication
- Session management

### ğŸ’¬ Chat Interface
- Real-time streaming responses
- Customizable model parameters
- Message history management
- Responsive design with dark/light mode support
- Mathematical formula rendering with KaTeX
- Code syntax highlighting
- GitHub-flavored markdown support

### ğŸ¤– Comprehensive Model Management
- Browse and manage local models
- Pull new models from repositories
- Delete unused models
- View detailed model information
- Create and customize models
- Push models to registry
- Copy and modify existing models
- Monitor running model instances

### ğŸ”§ Additional Features
- Embeddings generation and management
- Server control panel
- Real-time error handling with toast notifications
- Version information
- Blob management
- Advanced parameter customization

## ğŸš€ Getting Started

### Prerequisites
- Node.js 20.x or later
- npm/yarn
- Ollama installed and running locally

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd ollama-ui
```

2. Install dependencies:
```bash
npm install
# or
yarn install
```

3. Create a `.env` file:
```env
OLLAMA_API_HOST=http://localhost:11434
```

4. Start the development server:
```bash
npm run dev
# or
yarn dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser

## ğŸ— Project Structure

```
ollama-ui/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                # Next.js app router pages
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ chat/           # Chat interface
â”‚   â”‚   â”œâ”€â”€ models/         # Model management
â”‚   â”‚   â”œâ”€â”€ settings/       # Application settings
â”‚   â”‚   â”œâ”€â”€ blobs/          # Blob management
â”‚   â”‚   â”œâ”€â”€ embeddings/     # Embeddings generation
â”‚   â”‚   â”œâ”€â”€ version/        # Version information
â”‚   â”‚   â”œâ”€â”€ list-models/    # Model listing
â”‚   â”‚   â”œâ”€â”€ model-info/     # Model details
â”‚   â”‚   â”œâ”€â”€ pull-model/     # Model pulling
â”‚   â”‚   â”œâ”€â”€ push-model/     # Model pushing
â”‚   â”‚   â”œâ”€â”€ copy-model/     # Model copying
â”‚   â”‚   â”œâ”€â”€ create-model/   # Model creation
â”‚   â”‚   â”œâ”€â”€ delete-model/   # Model deletion
â”‚   â”‚   â””â”€â”€ running-models/ # Running model instances
â”‚   â”œâ”€â”€ components/         # Reusable UI components
â”‚   â”œâ”€â”€ hooks/              # Custom React hooks   
â”‚   â”œâ”€â”€ lib/                # Utility functions
â”‚   â”œâ”€â”€ store/              # State management (Zustand)
â”‚   â””â”€â”€ types/              # TypeScript definitions
â”œâ”€â”€ __tests__/              # Test files
â”œâ”€â”€ public/                 # Static assets
â””â”€â”€ ...                     # Config files
```

## ğŸ›  Tech Stack

- **Framework:** Next.js 15.1.7 with App Router and TurboPack
- **Language:** TypeScript 5
- **Authentication:** JSON Web Tokens (JWT)
- **Styling:** 
  - Tailwind CSS 3.4.1
  - CSS Animations
  - Dark/Light mode support
- **UI Components:** 
  - Radix UI primitives
  - Framer Motion animations
  - Sonner toast notifications
  - shadcn/ui components
- **State Management:** 
  - Zustand
  - SWR for data fetching
- **Content Rendering:**
  - React Markdown
  - KaTeX for math formulas
  - Syntax highlighting
  - GitHub-flavored markdown
- **Testing:**
  - Jest
  - React Testing Library
  - Coverage reporting
- **Data Validation:** Zod schema validation
- **Development Tools:**
  - ESLint
  - TypeScript strict mode
  - TurboPack

## ğŸ§ª Testing

The project includes comprehensive testing:

```bash
# Run all tests
npm test

# Watch mode for development
npm run test:watch

# Generate coverage report
npm run test:coverage
```

Test files are located in the `__tests__` directory and follow the `.test.ts(x)` naming convention.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is privately licensed. All rights reserved.

---

<div align="center">
Made with â¤ï¸ to lower the barrier for those wanting to learn and play with AI

[Report Bug](https://github.com/username/ollama-ui/issues) Â· [Request Feature](https://github.com/username/ollama-ui/issues)
</div>

## âš™ï¸ Environment Configuration

### Required Environment Variables

The application uses environment variables for configuration. Create a `.env` file in the root directory with the following variables:

```env
# Required - Ollama API endpoint
OLLAMA_API_HOST=http://localhost:11434

# Required - Authentication (if enabled)
JWT_SECRET=your-secret-key-here
AUTH_ENABLED=true

# Optional - Server configuration
NODE_ENV=development
PORT=3000
```

### Environment Files

- `.env`: Main environment file for local development
- `.env.test`: Environment configuration for testing
- `.env.example`: Example configuration (do not use in production)

### Configuration Options

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `OLLAMA_API_HOST` | Ollama API endpoint URL | http://localhost:11434 | Yes |
| `JWT_SECRET` | Secret key for JWT signing | - | Yes (if auth enabled) |
| `AUTH_ENABLED` | Enable/disable authentication | false | No |
| `NODE_ENV` | Node environment (development/production/test) | development | No |
| `PORT` | Port for the Next.js application | 3000 | No |

### Usage in Different Environments

1. **Local Development**
   ```bash
   # Use default .env file
   npm run dev
   ```

2. **Testing**
   ```bash
   # Use .env.test configuration
   npm run test
   ```

3. **Production**
   ```bash
   # Use production environment variables
   npm run build
   npm run start
   ```

### Security Considerations

- Never commit `.env` files containing sensitive information
- In production, use secure HTTPS URLs for `OLLAMA_API_HOST`
- Consider using a secrets manager for production deployments